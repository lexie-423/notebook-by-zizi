# Chap 1 引言

## 1. 机器学习的组件

### A. 数据

- 样本/数据点/数据实例->数据，拥有特征/协变量->属性
- 【引用概统中的概念】独立同分布
- 当每个样本的特征类别数量都是相同的时候，其特征向量是固定长度的，这个长度被称为数据的维数（dimensionality）

实际处理中，每个数据的来源不一样，会导致特征向量的长度不一样(比如分辨率不一样的图片)。传统机器学习只能处理一个长度的数据，而深度学习的其中一个优势就是可以处理不同长度的数据。

### B. 模型

### C. 目标函数

和数模的概念比较像，类似一个量化的优化目标，深度学习里面主要是 `min`，一般是 **损失函数** (loss fuction / cost function, 字面意思，挺好理解 orz )

函数类型：主要是最小化错误率，比如说最简单的有平方误差（方差?）

函数定义：由模型定义的，取决于模型参数。

这里有引入两个概念，训练数据集和训练集，形象一点可以理解成平时的作业和考试orz

一般训练数据集用来拟合模型参数(造函数)，测试数据集用于评估拟合的模型(检验函数)

**过拟合：** 当一个模型在训练集上表现良好，但不能推广到测试集时是过拟合。

### D. 优化算法

用于搜索出最佳参数来最小化损失函数。
用的最多的：梯度下降(gradient descent)

## 2 各种机器学习问题

### 2.1 监督学习

目标：生成一个模型，能够将任何输入特征映射到标签（即预测）

适用范围：在给定一组特定的可用数据的情况下，估计未知事物的概率，比如：

- 根据计算机断层扫描肿瘤图像，预测是否为癌症；
- 给出一个英语句子，预测正确的法语翻译；
- 根据本月的财务报告数据，预测下个月股票的价格；

方式：根据已有标签的数据集进行训练，来预测新的数据集的标签

具体步骤：

1. 从已知大量数据样本中随机选取一个子集，为每个样本获取真实标签。有时，这些样本**已有标签**（例如，患者是否在下一年内康复？）；有时，这些样本可能需要被**人工标记**（例如，图像分类）。这些输入和相应的标签一起构成了训练数据集；

2. 选择有监督的学习算法，它将训练数据集作为输入，并输出一个“已完成学习的模型”；

3. 将之前没有见过的样本特征放到这个“已完成学习的模型”中，使用模型的输出作为相应标签的预测。

#### 2.1.1 回归 -- “有多少”

感觉更倾向于优化/求解一类的问题，和之前做的数模题一个道理，主要就是平方误差损失函数最小化

高中时候学了“线性回归”的思想

#### 2.1.2 “分类” -- 是哪类

核心目标：训练一个分类器来确认类别

衡量标准：“不确定性”，常常用 **交叉熵** 来衡量

类别：
- 两个以上的分类：多项分类(multiclass classification)
- 层次分类：寻找层次结构（树--生物）

#### 2.1.3 标注

学习预测不相互排斥的类别问题称为多标签分类。
用途例如：描述输入图像的内容

#### 2.1.4 搜索

信息检索领域-网页排序问题-Pagerank

#### 2.1.5 推荐系统

推荐系统会给“给定用户和产品的匹配性”打分，可能是估计评级或购买概率

问题：推荐系统会造成反馈循环：学习->反馈->推荐相应类别->形成正反馈
#### 2.1.6 序列学习

序列问题的特征：输入和输出是不同长度的序列
核心功能：摄取输入序列，得到输出序列
领域：标记和解析 | 自动语音识别 | 文本到语音 | 机器翻译

### 2.2 无监督学习

**核心: 没有给出的特征和标签，只有奖励**-- 没有“目标”的机器学习

- 聚类问题：把相似值分成一类
- 主成分分析：捕捉物品线性相关的属性
- 因果关系和概率图模型：数据关系的联系
- 生成对抗网络

### 2.3 与环境互动

之前：离线学习：与环境断开
与真实环境互动：

![|525](image/Pasted%20image%2020250218085808.png)

分布偏移：研究：环境是否变化？例如，未来的数据是否总是与过去相似，还是随着时间的推移会发生变化？是自然变化还是响应我们的自动化工具而发生变化？

### 2.4 强化学习

**概念：** 在强化学习问题中，智能体（agent）在一系列的时间步骤上与环境交互

![](image/Pasted%20image%2020250218085949.png)

由奖励引出的：学分分配(credit assignment)问题：
1. 由于唯一真正的奖励出现在游戏结束的时候，中间智能体必须反思什么是奖励、什么是惩罚
2. 在任何时间点上，强化学习智能体可能知道一个好的策略，但可能有许多更好的策略从未尝试过的。 强化学习智能体必须不断地做出选择：是应该利用当前最好的策略，还是探索新的策略空间（放弃一些短期回报来换取知识）
3. 当环境可被完全观察到时，强化学习问题被称为**马尔可夫决策过程**（markov decision process）。 当状态不依赖于之前的操作时，我们称该问题为**上下文赌博机/老虎机**（contextual bandit problem）。 当没有状态，只有一组最初未知回报的可用动作时，这个问题就是经典的**多臂赌博机/老虎机**（multi-armed bandit problem）

## 3 发展历程

具体的领域前沿可以重新翻书，感觉有意思的思想主要是两个：

制约人工智能发展的两个方面：
内存效率；算力

一开始，算力小，内存小，则研究对算力要求小（理论依据更强）的算法
后来数据规模变大、算力变大，而内存不够了：
- 提交内存效率
- 能够通过叠算力：深度神经网络/多层感知机/长短期记忆网络/Q学习，利用大算力->准确结果
- 减少训练过程中需要存储的量（例如注意力机制，存储指针 instead of 存储数据）




