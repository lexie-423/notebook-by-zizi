好的，这是根据您提供的演示文稿（PPT）整理的一份详细的简体中文学习笔记。所有数学公式均使用 $...$ 格式。

---

## 支持向量机 (Support Vector Machines, SVM) 学习笔记

**主讲人**: 胡浩基 (Prof. Haoji Hu)
**单位**: 浙江大学信息与电子工程学院

### 1. 历史背景

支持向量机（SVM）是机器学习发展史上的一个重要里程碑。根据时间线，它在1995年被提出，处于连接传统机器学习算法（如反向传播）和现代深度学习浪潮的关键时期。



- **1986年**: 人工神经网络反向传播算法。
- **1995年**: **支持向量机 (SVM)** 诞生。
- **2006年**: Hinton 提出深度学习。

### 2. 基本定义

#### 2.1. 线性可分与线性不可分

- **线性可分数据集 (Linear Separable Dataset)**: 对于一个二分类数据集，如果存在一条直线（在高维空间中则为超平面）能将两个类别完全分开，那么这个数据集就是线性可分的。
- **线性不可分数据集 (Non-Linear Separable Dataset)**: 如果不存在这样的直线（或超平面），那么这个数据集就是线性不可分的。

| 二维空间示例 | 三维空间示例 |
| :---: | :---: |
|  |  |
| (左) 线性可分, (右) 线性不可分 | (左) 线性可分, (右) 线性不可分 |

### 3. 数学表达与定义

#### 3.1. 超平面方程

在二维空间中，一条直线可以用以下方程表示：
$w_1x_1 + w_2x_2 + b = 0$
- $w_1$ 和 $w_2$ 是维度 $x_1$ 和 $x_2$ 的**权重 (weights)**。
- $b$ 被称为**偏置 (bias)**。

这条线将平面分为两个区域：
- 对于一侧的点（例如，圆圈），满足：$w_1x_1 + w_2x_2 + b > 0$
- 对于另一侧的点（例如，叉），满足：$w_1x_1 + w_2x_2 + b < 0$

#### 3.2. 训练样本的定义

我们使用以下方式严格定义训练样本：
- 训练数据集表示为 $\{(X_i, y_i)\}_{i=1 \sim N}$，其中 $N$ 是样本总数。
- $X_i$ 是一个特征向量。例如，在 $d$ 维空间中，$X_i = [x_{i1}, x_{i2}, \dots, x_{id}]^T$。
- $y_i$ 是类别标签，为了便于推导，SVM 中通常定义为：
  $y_i = \begin{cases} +1, & \text{if } X_i \in C_1 \\ -1, & \text{if } X_i \in C_2 \end{cases}$

#### 3.3. 线性可分的紧凑定义

使用向量表示法，超平面方程为 $w^T X + b = 0$，其中 $w$ 是权重向量，$X$ 是特征向量。

一个数据集 $\{(X_i, y_i)\}_{i=1 \sim N}$ 是线性可分的，如果存在一个 $(w, b)$ 使得对于所有的样本 $i=1 \sim N$ 都满足：
- 如果 $y_i = +1$，则 $w^T X_i + b > 0$
- 如果 $y_i = -1$，则 $w^T X_i + b < 0$

这两个条件可以合并为一个更紧凑的表达式：
$y_i(w^T X_i + b) > 0 \quad (i = 1 \sim N)$

### 4. SVM 的核心思想：最大化间隔

对于一个线性可分的数据集，可能存在无数条线可以将两类数据分开。SVM 的目标是找到**最好**的那一条。



- **支持向量 (Support Vectors)**: 距离分割超平面最近的那些样本点。
- **间隔 (Margin)**: 两类数据的支持向量到分割超平面的距离之和。它就像是两条平行于分割超平面、且穿过支持向量的虚线之间的“街道”宽度。

**SVM 的核心思想**：寻找一个超平面，使得这个**间隔最大化**。



最优的超平面应具备以下三个性质：
1.  正确地将两个类别分开。
2.  在所有可以分开两个类别的超平面中，它具有最大的间隔。
3.  它正好位于支持向量构成的“街道”的中间。

### 5. SVM 的优化问题 (硬间隔)

为了找到最大间隔的超平面，我们需要解决以下优化问题：

- **目标函数 (Minimize)**：
  $\frac{1}{2} ||w||^2$

- **约束条件 (Subject to)**：
  $y_i[w^T X_i + b] \ge 1 \quad (i = 1 \sim N)$

**为什么是这个形式？**
1.  **距离公式**: 任意点 $X_0$ 到超平面 $w^T X + b = 0$ 的距离为 $d = \frac{|w^T X_0 + b|}{||w||}$。
2.  **缩放不变性**: 超平面 $(w, b)$ 和 $(aw, ab)$ (其中 $a>0$) 是同一个平面。我们可以利用这个性质对 $(w, b)$ 进行缩放，使得所有支持向量都满足 $|w^T X_{sv} + b| = 1$。
3.  **推导**:
    - 在这个缩放下，支持向量到超平面的距离为 $d_{sv} = \frac{1}{||w||}$。
    - 总间隔 (Margin) 为 $2 \times d_{sv} = \frac{2}{||w||}$。
    - **最大化间隔** $\frac{2}{||w||}$ 等价于**最小化** $||w||$。
    - 最小化 $||w||$ 等价于最小化 $\frac{1}{2}||w||^2$ (这里的 $\frac{1}{2}$ 和平方是为了后续求导方便)。
4.  **约束的意义**: $y_i[w^T X_i + b] \ge 1$ 确保了所有样本点都被正确分类，并且都位于间隔边界之外（或之上）。

此优化问题是一个**凸优化**问题中的**二次规划 (Quadratic Programming, QP)** 问题，因为目标函数是二次项，约束条件是线性的。这意味着它只有一个全局最优解。

### 6. 软间隔SVM：处理线性不可分数据

当数据线性不可分时，我们允许一些样本点被错分或者进入间隔区域。为此，引入**松弛变量 (slack variables)** $\delta_i \ge 0$。

#### 6.1. L1-SVM

- **目标函数 (Minimize)**：
  $\frac{1}{2}||w||^2 + C \sum_{i=1}^{N} \delta_i$

- **约束条件 (Subject to)**：
  1. $y_i[w^T X_i + b] \ge 1 - \delta_i$
  2. $\delta_i \ge 0 \quad (i = 1 \sim N)$

- **超参数 $C$**: 这是一个预定义的惩罚系数。
  - $C$ 很大时，模型倾向于不容忍错分点，可能会导致过拟合。
  - $C$ 很小时，模型对错分点的容忍度更高，可能导致欠拟合。

#### 6.2. L2-SVM

只需将目标函数中的惩罚项改为平方项：
- **目标函数 (Minimize)**：
  $\frac{1}{2}||w||^2 + C \sum_{i=1}^{N} \delta_i^2$

### 7. 核技巧 (The Kernel Trick)

对于非线性问题，除了使用软间隔，更强大的方法是将数据从低维空间映射到高维空间，使其在高维空间中变得线性可分。

- **映射函数**: 设 $\phi(X)$ 是一个将数据从原始特征空间映射到更高维空间的函数。
- **优化问题**: 原始的优化问题变为在新的特征空间中进行：
  $\min_{w,b,\delta} \frac{1}{2}||w||^2 + C \sum \delta_i$
  s.t. $y_i[w^T \phi(X_i) + b] \ge 1 - \delta_i$

#### 7.1. 核函数

直接进行高维映射和计算可能非常复杂甚至不可行（例如映射到无限维）。**核技巧**的精髓在于：我们不需要知道 $\phi(X)$ 的具体形式，只需要能够计算映射后特征向量的内积 $\phi(X_i)^T \phi(X_j)$ 即可。

我们定义**核函数 (Kernel Function)** 为：
$K(X_i, X_j) = \phi(X_i)^T \phi(X_j)$

- **Mercer定理**: 一个函数能成为有效的核函数的充要条件是它满足对称性且其核矩阵是半正定的。

#### 7.2. 常见核函数

1.  **线性核 (Linear Kernel)**:
    $K(X_1, X_2) = X_1^T X_2$
    （相当于没有使用核技巧）
2.  **高斯核 (Gaussian Kernel / RBF Kernel)**:
    $K(X_1, X_2) = \exp(-\frac{||X_1 - X_2||^2}{2\sigma^2})$
    （$\sigma$ 是超参数；可映射到无限维空间）
3.  **多项式核 (Polynomial Kernel)**:
    $K(X_1, X_2) = (X_1^T X_2 + C)^d$
    （$C, d$ 是超参数）
4.  **Sigmoid 核**:
    $K(X_1, X_2) = \tanh(\beta X_1^T X_2 + C)$
    （$\beta, C$ 是超参数）

### 8. 拉格朗日对偶性与SVM对偶问题

直接求解带约束的SVM原始问题很困难。我们通过**拉格朗日对偶性**来求解。

#### 8.1. 理论回顾

- **原始问题 (Primal Problem)**: 我们最初想要解决的优化问题。
- **拉格朗日函数**: 引入拉格朗日乘子（对偶变量），将约束条件加入目标函数。
- **对偶问题 (Dual Problem)**: 原始问题的另一种数学形式，通常更容易求解。
- **强对偶性**: 在满足一定条件时（如SVM是凸优化问题），原始问题和对偶问题的最优解是相等的。
- **KKT条件**: 强对偶性成立时，最优解必须满足的一系列条件。其中一个重要条件是**互补松弛性 (complementary slackness)**。

#### 8.2. SVM对偶问题的推导与形式

对软间隔SVM的原始问题应用拉格朗日对偶性，经过一系列推导，我们可以得到其**对偶问题**：

- **目标函数 (Maximize)**：
  $\theta(\alpha) = \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j K(X_i, X_j)$

- **约束条件 (Subject to)**：
  1. $0 \le \alpha_i \le C \quad (i=1, 2, ..., N)$
  2. $\sum_{i=1}^{N} \alpha_i y_i = 0$

- **对偶变量**: $\alpha_i$ 是每个样本对应的拉格朗日乘子。

**优势**:
- 优化变量从 $d$ (特征维度) 变为 $N$ (样本数量)，当特征维度很高时更高效。
- 目标函数中只依赖于核函数 $K(X_i, X_j)$，完美地融入了核技巧。

### 9. 求解与分类

#### 9.1. 训练过程 (L1-SVM)

1.  **求解 $\alpha_i$**: 解决上述对偶二次规划问题，得到最优的 $\alpha_i$。
    - 大多数 $\alpha_i$ 会等于0，只有**支持向量**对应的 $\alpha_i$ 才大于0。
2.  **计算 $b$**: 选择一个满足 $0 < \alpha_i < C$ 的支持向量 $X_i$，根据 KKT 条件，它必须满足 $y_i(w^T \phi(X_i) + b) = 1$。由此可以解出 $b$：
   $b = y_i - w^T \phi(X_i) = y_i - \sum_{j=1}^{N} \alpha_j y_j K(X_j, X_i)$
   为提高稳健性，通常会对所有支持向量计算出的 $b$ 取平均值。

#### 9.2. 测试 (分类) 过程

对于一个新的数据点 $X_{test}$，其类别由以下决策函数决定：
$f(X_{test}) = w^T \phi(X_{test}) + b = \sum_{i=1}^{N} \alpha_i y_i K(X_i, X_{test}) + b$
预测的类别为 $\text{sign}(f(X_{test}))$。

### 10. 模型评估

- **混淆矩阵 (Confusion Matrix)**: 展示了模型预测的正确和错误情况，包含真正例(TP), 假正例(FP), 真反例(TN), 假反例(FN)。
- **ROC曲线 (Receiver Operating Characteristic Curve)**:
  - 以**假正例率 (FPR = FP / (FP+TN))** 为横轴，以**真正例率 (TPR = TP / (TP+FN))** 为纵轴绘制的曲线。
  - 曲线越靠近左上角，表示模型性能越好。
- **EER (Equal Error Rate)**: 当 FPR = 1 - TPR (即错误接受率 = 错误拒绝率) 时的错误率，是衡量系统性能的指标之一。

### 11. 多分类SVM

SVM本身是二分类器，要实现多分类需要特殊策略：
1.  **一对多 (One-vs-Rest, OVR)**: 为 $N$ 个类别训练 $N$ 个分类器，每个分类器负责将一个类别与其余所有类别分开。
2.  **一对一 (One-vs-One, OVO)**: 为 $N$ 个类别两两组合，训练 $\frac{N(N-1)}{2}$ 个分类器。预测时采用投票法。

---